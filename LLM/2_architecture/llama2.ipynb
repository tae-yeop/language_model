{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4671889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87677ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub version: 0.32.3\n",
      "sentencepiece version: 0.2.0\n",
      "torch version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    \"sentencepiece\",    # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0976f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb86062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        irms = x.pow(2).mean(-1, keepdim=True)\n",
    "        return self.weight * x * torch.rsqrt(irms + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b21d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.randn(2,3,4)\n",
    "\n",
    "rms_norm = RMSNorm(dim=test_tensor.size(-1))\n",
    "rms_norm_torch = nn.RMSNorm(test_tensor.size(-1), eps=1e-5)\n",
    "\n",
    "assert torch.allclose(rms_norm(test_tensor), rms_norm_torch(test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82013fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c2c69ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "silu = SiLU()\n",
    "\n",
    "assert torch.allclose(silu(test_tensor), nn.SiLU()(test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2334b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaFeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # 32 bit를 올려서 컨버팅하는것 보단 dtype 바로 하는게 메모리 덜씀\n",
    "        self.fc1 = nn.Linear(cfg.emb_dim, cfg.hidden_dim, dtype=cfg.dtype, bias=False)\n",
    "        self.fc2 = nn.Linear(cfg.emb_dim, cfg.hidden_dim, dtype=cfg.dtype, bias=False)\n",
    "        self.fc3 = nn.Linear(cfg.hidden_dim, cfg.emb_dim, dtype=cfg.dtype, bias=False)\n",
    "        self.silu = SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.silu(self.fc1(x))\n",
    "        swiglu_out = self.fc2(x) * gate\n",
    "        x = self.fc3(swiglu_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6aa26c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0200, 0.0400, 0.0600, 0.0800, 0.1000, 0.1200, 0.1400, 0.1600,\n",
       "        0.1800, 0.2000, 0.2200, 0.2400, 0.2600, 0.2800, 0.3000, 0.3200, 0.3400,\n",
       "        0.3600, 0.3800, 0.4000, 0.4200, 0.4400, 0.4600, 0.4800, 0.5000, 0.5200,\n",
       "        0.5400, 0.5600, 0.5800, 0.6000, 0.6200, 0.6400, 0.6600, 0.6800, 0.7000,\n",
       "        0.7200, 0.7400, 0.7600, 0.7800, 0.8000, 0.8200, 0.8400, 0.8600, 0.8800,\n",
       "        0.9000, 0.9200, 0.9400, 0.9600, 0.9800])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 101, 2)[:50] / 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope_params(dim, theta_base=10_000, context_length=4096):\n",
    "    assert dim % 2 == 0, \"embedding dim must be even\"\n",
    "\n",
    "    # inverse freqs\n",
    "\n",
    "    # dim이 짝수라고 가정\n",
    "    exponent = torch.arange(0, dim, 2)[: dim // 2].float() / dim  # shape: (dim//2,)\n",
    "\n",
    "    # theta_base가 파이썬 float라면 → 연산 순간 0-차원(scalar) Tensor로 승격\n",
    "    # shape: ()\n",
    "    power_term = theta_base ** exponent  # broadcasting: () vs (dim//2,) → 결과 shape (dim//2,)\n",
    "\n",
    "    inv_freqs = 1.0 / power_term        # 1.0 역시 scalar → 최종 shape (dim//2,)\n",
    "\n",
    "    \n",
    "    # position ids\n",
    "    position = torch.arange(context_length)\n",
    "\n",
    "    # angles\n",
    "    angles = position[:, None] * inv_freqs[None, :]  # shape: (context_length, dim//2)\n",
    "\n",
    "    # expand angles\n",
    "    angles = torch.cat([angles, angles], dim=1) # shape : (context_length, dim)\n",
    "\n",
    "    # precomptue cos and sin\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rope(x, cos, sin):\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    x1 = x[..., : head_dim // 2] # first half\n",
    "    x2 = x[..., head_dim // 2:] # second half\n",
    "\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0) # shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0) # shape: (1, 1, seq_len, head_dim)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc89edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "context_len = 5\n",
    "num_heads = 4\n",
    "head_dim = 16\n",
    "\n",
    "cos, sin = precompute_rope_params(dim=head_dim, context_length=context_len)\n",
    "\n",
    "queries = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
    "keys = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
    "\n",
    "queries_rot = compute_rope(queries, cos, sin)\n",
    "keys_rot = compute_rope(keys, cos, sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e00cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, num_heads, dtype=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        self.to_qkv = nn.Linear(d_in, 3 * d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length, dtype=torch.bool), diagonal=1))\n",
    "\n",
    "        cos, sin = precompute_rope_params(dim=head_dim, context_length=context_length)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        q, k, v = self.to_qkv(x).split(self.d_out, dim=-1)\n",
    "        q = q.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        k = compute_rope(k, self.cos, self.sin)\n",
    "        q = compute_rope(q, self.cos, self.sin)\n",
    "\n",
    "        attn_scores = q @ k.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / k.shape[-1]**0.5, dim=-1)\n",
    "        \n",
    "        context_vec = (attn_weights @ v).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43582665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg.emb_dim,\n",
    "            d_out=cfg.emb_dim,\n",
    "            context_length=cfg.context_length,\n",
    "            num_heads=cfg.n_heads,\n",
    "            dtype=cfg.dtype\n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = RMSNorm(cfg.emb_dim)\n",
    "        self.norm2 = RMSNorm(cfg.emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residue = x\n",
    "        x = self.norm1(x)\n",
    "        x = attn(x)\n",
    "        x = x = residue\n",
    "\n",
    "        residue = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x + residue\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79688f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama2Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.emb_dim, cfg.dtype)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg.n_layers)]\n",
    "        )\n",
    "\n",
    "        self.final_norm = RMSNorm(cfg.emb_dim)\n",
    "        self.out_head = nn.Linear(cfg.emb_dim, cfg.vocab_size, bias=False, dtype=cfg.dtype)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # batch_size, seq_len = in_idx.shape\n",
    "        tok_embs = self.tok_emb(in_idx)\n",
    "        x = tok_embs\n",
    "        x = self.blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Llama2Config:\n",
    "    vocab_size: int = 32000\n",
    "    context_length: int = 4096\n",
    "    emb_dim: int = 4096\n",
    "    n_heads: int = 32\n",
    "    n_layers: int = 32\n",
    "    hiddem_dim = 1108\n",
    "    dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159fe8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Llama2Config()\n",
    "model = Llama2Model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7167bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efe002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "tokenizer_file = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b\",\n",
    "    filename=\"tokenizer.model\",\n",
    "    local_dir=\"Llama-2-7b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49990fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "class LlamaTokenizer:\n",
    "    def __init__(self, tokenizer_file):\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.load(tokenizer_file)\n",
    "        self.tokenizer = sp\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode_as_ids(text)\n",
    "    def decode(self, ids):\n",
    "        return self.tokenizer.decode_pieces(ids)\n",
    "    \n",
    "    b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da54ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # Add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        #  Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # Apply temperature\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            \n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a43e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves\", tokenizer).to(device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=cfg.context_length,\n",
    "    top_k=1,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba89e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b\",\n",
    "    filename=\"consolidated.00.pth\",\n",
    "    local_dir=\"Llama-2-7b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e88429",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(weights_file, weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(weights.keys())[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d841517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {left.shape} vs {right.shape}\")\n",
    "    \n",
    "    if isinstance(right, torch.Tensor):\n",
    "        return nn.Parameter(right.clone().detach())\n",
    "    else:\n",
    "        return nn.Parameter(torch.tensor(right))\n",
    "    \n",
    "\n",
    "def load_weights_into_llama(model, param_config, params):\n",
    "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"tok_embeddings.weight\"])\n",
    "\n",
    "    for l in range(param_config.n_layers):\n",
    "        \n",
    "        # Attnention weights\n",
    "        model.blocks[l].att.to_qkv.weight = assign(\n",
    "            model.blocks[l].att.W_query.weight, params[f\"layers.{l}.attention.wq.weight\"]\n",
    "        )\n",
    "        \n",
    "        model.blocks[l].att.W_key.weight = assign(\n",
    "            model.blocks[l].att.W_key.weight, params[f\"layers.{l}.attention.wk.weight\"]\n",
    "        )\n",
    "\n",
    "        model.blocks[l].att.W_value.weight = assign(\n",
    "            model.blocks[l].att.W_value.weight, params[f\"layers.{l}.attention.wv.weight\"]\n",
    "        )\n",
    "\n",
    "        model.blocks[l].att.out_proj.weight = assign(\n",
    "            model.blocks[l].att.out_proj.weight, params[f\"layers.{l}.attention.wo.weight\"]\n",
    "        )\n",
    "\n",
    "        model.blocks[l].norm1.weight = assign(\n",
    "            model.blocks[l].norm1.weight, params[f\"layers.{l}.attention_norm.weight\"]\n",
    "        )\n",
    "\n",
    "        # Feed Forward weights\n",
    "        model.blocks[l].ff.fc1.weight = assign(\n",
    "            model.blocks[l].ff.fc1.weight, params[f\"layers.{l}.feed_forward.w1.weight\"]\n",
    "        )\n",
    "\n",
    "        # For some reason w2 and w3 are provided in the wrong order in the weights file\n",
    "        model.blocks[l].ff.fc2.weight = assign(\n",
    "            model.blocks[l].ff.fc2.weight, params[f'layers.{l}.feed_forward.w3.weight']\n",
    "        )\n",
    "\n",
    "        model.blocks[l].ff.fc3.weight = assign(\n",
    "            model.blocks[l].ff.fc3.weight, params[f'layers.{l}.feed_forward.w2.weight']\n",
    "        )\n",
    "\n",
    "        model.blocks[l].norm2.weight = assign(\n",
    "            model.blocks[l].norm2.weight, params[f'layers.{l}.ffn_norm.weight']\n",
    "        )\n",
    "\n",
    "    # Load output layer weights\n",
    "    model.final_norm.weight = assign(model.final_norm.weight, params[\"norm.weight\"])\n",
    "    model.out_head.weight = assign(model.out_head.weight, params[\"output.weight\"])\n",
    "\n",
    "\n",
    "load_weights_into_llama(model, cfg, weights)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449bd709",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=cfg.context_length,\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruction-finetuned model\n",
    "\n",
    "weights_file = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b-chat\",\n",
    "    filename=\"consolidated.00.pth\",\n",
    "    local_dir=\"Llama-2-7b-chat\"\n",
    ")\n",
    "\n",
    "model = Llama2Model(cfg)\n",
    "weights = torch.load(weights_file, weights_only=True)\n",
    "load_weights_into_llama(model, config, weights)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f406442",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"What do llamas eat?\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5e72ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
      "Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7f8b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobfile version: 3.0.0\n",
      "huggingface_hub version: 0.32.0\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.4.0a0+3bcc3cddb5.nv24.7\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"blobfile\",         # to download pretrained weights\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    \"tiktoken\",         # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d82c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10f5b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg.emb_dim, cfg.hidden_size, dtype=cfg.dtype, bias=False)\n",
    "        self.fc2 = nn.Linear(cfg.emb_dim, cfg.hidden_size, dtype=cfg.dtype, bias=False)\n",
    "        self.fc3 = nn.Linear(cfg.hidden_size, cfg.emb_dim, dtype=cfg.dtype, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = F.silu(self.fc1(x))\n",
    "        b = self.fc2(x)\n",
    "        c = gate * b\n",
    "        return self.fc3(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63d1c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_context_length = 8192\n",
    "low_freq_factor = 1.0\n",
    "high_freq_factor = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce4653b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_freq_wavelen = original_context_length / low_freq_factor\n",
    "high_freq_wavelen = original_context_length / high_freq_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c0dd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_freq_wavelen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d835e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2cc7730",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1771613717.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 19\u001b[0;36m\u001b[0m\n\u001b[0;31m    positions = torch.\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# modified RoPE\n",
    "# base 값이 500_000으로 증가 -> 차원마다 더 천천히 주파수(회전 각)가 줄어듬\n",
    "def compute_rope_params(head_dim, theta_base=10_000.0, max_seq_len=4096, scaling_cfg=None, dtype=torch.float32):\n",
    "    assert head_dim % 2 == 0\n",
    "    \n",
    "    # a) 기본 각 주파수(w_k) 계산 : rad / token\n",
    "    # k = 0, 2, 4, ..., head_dim - 2 (짝수 인덱스만 사용)\n",
    "    k = torch.arange(0, head_dim, 2, dtype=dtype)\n",
    "    angular_freq = 1.0 / (theta_base ** (k.float() / head_dim)) # (head_dim//2. )\n",
    "\n",
    "    # b) NTK-Style 스케일으로 저주파 영역 압축\n",
    "    if scaling_cfg is not None:\n",
    "        angular_freq = _apply_ntk_scaling(\n",
    "            angular_freq,\n",
    "            cfg=scaling_cfg,\n",
    "            seq_len_original=max_seq_len,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "    # c) 위치별 절대각(θ = n·ω) → cos/sin LookUp\n",
    "    positions = torch.arange(max_seq_len, dtype=dtype) # (L,)\n",
    "    angles = positions[:, None] * angular_freq[None :] # (L, head_dim/2)\n",
    "    anlges = torch.cat([angles, angles], dim=1) # (L, head_dim)\n",
    "\n",
    "    return torch.cos(angles), torch.sin(angles)\n",
    "\n",
    "def _apply_ntk_scaling(\n",
    "        angular_freq,\n",
    "        cfg,\n",
    "        seq_len_original,\n",
    "        dtype\n",
    "):\n",
    "    lmb = (2 * torch.pi) / angular_freq # 파장: tokens / cycle\n",
    "\n",
    "    lmb_low_cut = seq_len_original / cfg.low_freq_factor\n",
    "    lmb_high_cut = seq_len_original / cfg.high_freq_factor\n",
    "\n",
    "    # 1) 저주파(λ > λ_low_cut) ➜ 주파수 factor 배 ↑\n",
    "    w_scaled = torch.where(lmb > lmb_low_cut, angular_freq / cfg.factor, angular_freq)\n",
    "\n",
    "    # 2) 중간 주파수 영역을 부드럽게 선형 보간\n",
    "    smooth_ratio = (seq_len_original / lmb - cfg.low_freq_factor) / (cfg.high_freq_factor - cfg.low_freq_factor)\n",
    "\n",
    "    w_interp = (1 - smooth_ratio) * (angular_freq / cfg.factor) +  smooth_ratio * angular_freq\n",
    "\n",
    "    is_mid_band = (lmb < = lmb_)\n",
    "\n",
    "\n",
    "def apply_rope(\n",
    "        x,\n",
    "        cos_lut,\n",
    "        sin_lut\n",
    "):\n",
    "    B, H, L, D = x.shape\n",
    "    assert D % 2 != 0\n",
    "\n",
    "    # split along dim-D\n",
    "    x_even, x_odd = x[..., : D//2], x[..., D//2:]\n",
    "\n",
    "    cos = cos_lut[:L, :].view(1, 1, L, D) # (1,1,L,D)\n",
    "    sin = sin_lut[:L, :].view(1, 1, L, D)\n",
    "\n",
    "    x_rot = torch.cat([-x_odd, x_even], dim=-1) # B, H, L, D\n",
    "    out = x * cos + x_rot * sin \n",
    "    return out.to(dtpye=x.dtype)\n",
    "\n",
    "\n",
    "\n",
    "def resacle_theta(theta_odl, context_length_old, context_length_new):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd325983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RoPE parameters\n",
    "\n",
    "llama_2_context_len = 4096\n",
    "llama_3_context_len = 8192\n",
    "\n",
    "llama_2_theta_base = 10_000\n",
    "llama_3_theta_base = 500_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e18f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_heads = 4\n",
    "head_dim = 16\n",
    "\n",
    "cos, sin = precompute_rope_params(\n",
    "    head_dim=head_dim,\n",
    "    theta_base=llama_3_theta_base,\n",
    "    context_length=llama_3_context_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b52fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = torch.randn(batch_size, num_heads, llama_3_context_len, head_dim)\n",
    "keys = torch.randn(batch_size, num_heads, llama_3_context_len, head_dim)\n",
    "\n",
    "# Apply rotary position embeddings\n",
    "queries_rot = compute_rope(queries, cos, sin)\n",
    "keys_rot = compute_rope(keys, cos, sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GQA(Grouped-query attention)\n",
    "# reduce the number of key and value projections by sharing them among multiple attention heads\n",
    "# Each attention head still has its unique query, but these queries attend to the same group of keys and values\n",
    "# matmul과 파라미터수를 줄이도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f0a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedBuffers:\n",
    "    _buffers = {}\n",
    "    @staticmethod\n",
    "    def get_buffers(context_length, head_dim, rope_base, freq_config, dtype=torch.float32):\n",
    "        key = (context_length, head_dim, rope_base, tuple(freq_config.values()) if freq_config else freq_config, dtype)\n",
    "\n",
    "        if key not in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ea6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_in,\n",
    "        d_out,\n",
    "        context_length,\n",
    "        num_heads,\n",
    "        num_kv_groups,\n",
    "        rope_base=10_000,\n",
    "        rope_config=None,\n",
    "        dtype=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "        mask, cos, sin = SharedBuffers.get_buffers(context_length, self.head_dim, rope_base, rope_config, dtype)\n",
    "\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2) # (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.transpose(1, 2) # (b, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2) # (b, num_query_groups, num_tokens, head_dim)\n",
    "\n",
    "        keys = compute_rope(keys, self.cos, self.sin)\n",
    "        queries = compute_rope(queries, self.cos, self.sin)\n",
    "\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = valeus.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        # \n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        assert keys.shape[-1] == self.head_dim\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.emb_dim, dtype=cfg.dtype)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg.n_layres)]\n",
    "        )\n",
    "\n",
    "        self.final_form = nn.RMSNorm(cfg.emb_dim, eps=1e-5, dtype=cfg.dtype)\n",
    "        self.out_head = nn.Linear(cfg.emb_dim, cfg.vocab_size, bias=False, dtype=cfg.dtype)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(cfg.context_length, cfg.context_length), diagonal=1).bool(), persistent=False\n",
    "        )\n",
    "\n",
    "        cfg.rope_base = rescale_theta(cfg.rope_base, cfg.orig_context_length, cfg.context_length)\n",
    "\n",
    "        cos, sin = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d996c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "# Llama 3, however, reverted back to using the BPE tokenizer from Tiktoken; specifically, it uses the GPT-4 tokenizer with an extended vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43202d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import Tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, model_path):\n",
    "        assert os.path.isfile(model_path), f\"Model file {model_path} not found\"\n",
    "        mergeable_ranks = load_tiktoken_bpe(model_path)\n",
    "\n",
    "        self.special_tokens = {\n",
    "            \"<|begin_of_text|>\": 128000,\n",
    "            \"<|end_of_text|>\": 128001,\n",
    "            \"<|start_header_id|>\": 128006,\n",
    "            \"<|end_header_id|>\": 128007,\n",
    "            \"<|eot_id|>\": 128009,\n",
    "        }\n",
    "\n",
    "        self.special_tokens.update({\n",
    "            f\"<|reserved_{i}|>\": 128002 + i for i in range(256) if (128002 + i) not in self.special_tokens.values()\n",
    "        })\n",
    "\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,\n",
    "            pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=self.special_tokens\n",
    "        )\n",
    "\n",
    "    def encode(self, text, bos=False, eos=False, allowed_special=set(), disallowed_special=()):\n",
    "        if bos:\n",
    "            tokens = [self.special_tokens[\"<|begin_of_text|>\"]]\n",
    "        else:\n",
    "            tokens = []\n",
    "\n",
    "        tokens += self.model.encode(text, allowed_special=allowed_special, disallowed_special=disallowed_special)\n",
    "\n",
    "        if eos:\n",
    "            tokens.append(self.special_tokens[\"<|end_of_text|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.model.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "tokenizer_file_path = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B\",\n",
    "    filename=\"original/tokenizer.model\",\n",
    "    local_dir=\"Llama-3-8B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba282f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llam3 files을 쓰려면 blobfile이 필요함. 클라우드 저장소에 있는 데이터셋, 모델을 다루는데 필요함\n",
    "# !pip install blobfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82efec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tokenizer_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2566d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=cfg.context_length,\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "combined_weights = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "    weights_file = hf_hub_download(\n",
    "        repo_id=\"meta-llama/Meta-Llama-3-8B\",\n",
    "        filename=f\"model-0000{i}-of-00004.safetensors\",\n",
    "        local_dir=\"Llama-3-8B\"\n",
    "    )\n",
    "    current_weights = load_file(weights_file)\n",
    "    combined_weights.update(current_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8a2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
